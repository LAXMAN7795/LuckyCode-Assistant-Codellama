# LuckyCode-Assistant (CodeLlama + Ollama + Gradio)

LuckyCode-Assistant is a local AI-powered code assistant built using **CodeLlama**, **Ollama**, and **Gradio**.  
It runs completely offline and is designed to help with programming tasks such as debugging, explaining concepts, generating code, and improving developer productivity.

---

## ğŸš€ Features

- Runs on your local machine using **Ollama**
- Powered by **CodeLlama**, Metaâ€™s coding-optimized LLM
- Custom model named **LuckyCode**
- Gradio-based interactive UI
- Supports conversation history
- Works offline (no API keys needed)

---

## ğŸ§  About CodeLlama

**CodeLlama** is a specialized version of Llama 2, fine-tuned specifically for programming.  
It supports many languages like Python, C++, JavaScript, Java, PHP, C#, Bash, and more.

### Key capabilities:
- Code generation & explanation  
- Code infilling  
- Long-context understanding  
- Multi-language support  
- Available in 7Bâ€“70B parameter variants  
- Open-access and free for research/commercial use  

---

## âŒ Why Not Llama-2?

Llama-2 is general-purpose and not optimized for code.  
Its limitations include:
- Weaker coding accuracy  
- Poor reasoning for complex problems  
- Small context window  
- High prompt sensitivity  

CodeLlama fixes these and performs much better for coding tasks.

---

## ğŸ› ï¸ Setup Instructions

### 1ï¸âƒ£ Install **Ollama**
Download and install Ollama from:
- https://ollama.com/download

### 2ï¸âƒ£ Pull the CodeLlama model
Run the following in your terminal:
- `ollama run codellama`

This downloads the base model.

### 3ï¸âƒ£ Create a Conda environment
- Create environment with Python 3.10  
- Activate it in VS Code for development  

### 4ï¸âƒ£ Create a custom model
Add a `modelfile` in the project to define:
- The base model (`codellama`)
- Temperature settings  
- System prompt for LuckyCode  

Then create your model using `ollama create`.

### 5ï¸âƒ£ Install dependencies  
Add required libraries such as:
- Gradio  
- LangChain  
- Requests  

Install them using your environment.

### 6ï¸âƒ£ Run the assistant  
Launch the Gradio app to access the UI in your browser.

---

## ğŸ“ Project Structure

LuckyCode-Assistant/
â”‚â”€â”€ modelfile
â”‚â”€â”€ app.py
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ README.md
â””â”€â”€ (your conda environment ignored)

---

## ğŸ”’ .gitignore Note

If your Conda environment is stored inside the project folder (e.g., `codeAssist/`),  
add it to `.gitignore` to avoid pushing it to GitHub.

---

## ğŸ“œ License

CodeLlama models are licensed under Metaâ€™s **Llama Community License**.

---

## ğŸ‘¤ Author

**Laxman Sannu Gouda** 
Creator of **LuckyCode â€“ AI Code Assistant**

---

â­ If you like this project, consider giving it a star on GitHub!
